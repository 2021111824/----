# 1. 梯度下降法（Gradient Descent）

是一种用于优化函数的迭代算法，常用于机器学习和深度学习模型的训练中。其目标是通过不断更新模型参数，**使损失函数逐渐达到最小值**，从而提升模型的预测精度。

### 梯度下降法的基本原理

1. **损失函数（Loss Function）**：在机器学习中，损失函数用于**衡量模型预测值与真实值的偏差**。梯度下降法的目标是找到一组参数，使损失函数达到**最小**值。

2. **梯度（Gradient）**：梯度是损失函数**关于参数的偏导数**，表示损失函数在各个参数上的变化率。梯度的方向指向**损失函数增大的方向**，**反方向则指向损失函数减小的方向**。

3. **参数更新**：梯度下降通过沿着损失函数**负梯度**方向更新参数，使损失函数的值逐步减小。参数更新的公式为：
   
<p align="center">
  <img src="./image/K1-1.png" alt="公式" />
</p>

   其中：
   - θ 表示参数    
   - α 表示学习率（learning rate），控制更新的步长  
   - ∇<sub>θ</sub>J(θ) 表示损失函数 J(θ) 对参数 θ 的梯度

   不断更新参数后，理论上可以找到使损失函数达到局部或全局最小值的参数值。

### 梯度下降法的类型

1. **批量梯度下降（Batch Gradient Descent）**：
   - 在每次更新中使用**所有训练数据**计算梯度。
   - **优点**：方向稳定，容易收敛到全局最小值。
   - **缺点**：计算开销大，特别是在数据量很大时。

2. **随机梯度下降（Stochastic Gradient Descent, SGD）**：
   - 每次更新仅**使用一个样本**来计算梯度。
   - **优点**：计算速度快，适合大规模数据；有助于跳出局部最优解。
   - **缺点**：方向不稳定，可能导致震荡，难以精确收敛到最优解。

3. **小批量梯度下降（Mini-Batch Gradient Descent）**：
   - 每次更新使用一个**小批量的数据样本**来计算梯度。
   - **优点**：在计算效率和收敛稳定性之间取得平衡，通常比SGD更稳定，且比批量梯度下降更高效。

### 学习率的选择

学习率（α）控制了每次**更新的步长**，对梯度下降的效果有很大影响：
- **学习率过大**：更新步长太大，可能导致**在最优点附近来回震荡**或直接**越过最优点**，***无法收敛***。
- **学习率过小**：更新步长太小，收敛速度慢，可能导致**训练时间过长**。

可以通过学习率调整策略（如学习率衰减或自适应学习率方法）来动态调整学习率，以提高训练效果。

### 常见的改进方法

1. **动量（Momentum）**：
   - 动量方法在更新时考虑了上一次梯度的影响，使**参数更新更平滑**，减少震荡。公式为：
    <p align="center">
        <img src="./image/K1-2.png" alt="公式" />
    </p>
   - 其中 𝛽  是动量系数，v 是速度向量。

2. **Adagrad**：
   - 适应性调整每个参数的学习率，使得在迭代过程中稀疏参数更新较大，而频繁参数更新较小。

3. **RMSprop**：
   - RMSprop通过在训练过程中动态调整学习率，使其适应不同特征的变化，解决了Adagrad在训练中学习率逐渐变小的问题。

4. **Adam（Adaptive Moment Estimation）**：
   - Adam结合了动量和RMSprop的优点，利用梯度的一阶和二阶动量来自适应调整学习率，是目前应用最广泛的优化算法之一。

### 梯度下降法的优缺点

**优点**：
- 简单易实现，适用于大部分模型和任务。
- 计算成本较低，尤其是随机梯度下降和小批量梯度下降。

**缺点**：
- 对初始参数敏感，可能导致陷入局部最优解。
- 依赖学习率的选择，学习率的设置难以平衡收敛速度和稳定性。
- 在鞍点或平坦区域可能出现停滞，导致收敛困难。

### 应用场景

梯度下降法广泛应用于机器学习和深度学习的模型训练中，如线性回归、逻辑回归、神经网络等，适合优化高维度、非线性的复杂函数。
<br />
<br />
<br />
<br />


# 2. 公平性反向传播
**公平性反向传播**是一种在训练机器学习模型时结合公平性约束的优化方法。它的目标是在模型学习过程中，通过反向传播调整模型参数，不仅**最小化预测误差**，还**改善**模型在特定敏感属性（如性别、种族、年龄等）上的**公平性表现**。这样可以避免模型在这些属性上产生偏差。

### 背景与动机

在很多任务中，模型会在不同群体上表现出不平等的预测性能，这种不公平性可能导致对某些群体的歧视。公平性反向传播的目标是**在反向传播过程中融入公平性度量**，以控制和减少偏差。常见的公平性度量包括：

1. **均衡机会（Equal Opportunity）**：不同群体间的正类召回率（True Positive Rate）相等。
2. **均衡误差（Equalized Odds）**：不同群体在真阳性率和假阳性率上相等。
3. **均衡预测率（Demographic Parity）**：不同群体的正预测率相等。

### 公平性反向传播的原理

在传统的神经网络训练中，模型通过**最小化损失函数**来优化性能，损失函数通常基于**预测误差**。公平性反向传播在此基础上引入一个公平性损失项。通过定义**总损失函数为预测误差和公平性损失的加权和**，模型可以在优化预测精度的同时降低不公平性：

<p align="center">
    <img src="./image/K2-1.png" alt="公式" />
</p>

其中：
- **Prediction Loss**：通常为交叉熵损失或均方误差，用于衡量预测误差。
- **Fairness Loss**：用于衡量模型的公平性偏差。可以根据不同的公平性定义（如均衡机会）计算。
- **λ（lambda）**：是一个权重参数，用于控制公平性和准确性之间的平衡。

通过这个损失函数，公平性反向传播在更新参数时会同时考虑预测误差和公平性损失，逐步逼近一个公平的模型。

### 公平性反向传播的步骤

1. **定义损失函数**：在传统损失函数中加入公平性损失。具体的公平性损失形式根据任务需求和敏感属性而定。
2. **前向传播**：计算模型输出和公平性度量，得到预测误差和公平性误差。
3. **计算总损失**：将预测损失和公平性损失结合为总损失。
4. **反向传播**：通过梯度下降法或其他优化算法更新模型参数，以同时**最小化预测误差和公平性误差**。
5. **迭代训练**：在每一轮训练中不断更新参数，使得模型逐步**逼近一个既准确又公平的状态**。

### 公平性反向传播的优势与挑战

**优势**：
- **减少偏差**：模型在训练时被鼓励减少在敏感属性上的偏差。
- **透明性**：引入公平性损失的模型可以更清晰地控制和量化公平性。
- **灵活性**：可以根据具体的公平性目标（如均衡机会）灵活选择不同的公平性损失函数。

**挑战**：
- **准确性损失**：提高公平性可能会导致模型准确性下降，因此需要在公平性与准确性之间平衡。
- **复杂性**：公平性反向传播需要定义和计算公平性度量，增加了训练的复杂性。
- **参数选择困难**：如何合理选择λ参数以达到理想的平衡是一个难点。

### 应用场景

公平性反向传播适用于可能涉及公平性问题的领域，如：
- **招聘系统**：避免对不同性别或种族的应聘者产生偏见。
- **信用评分**：在信用风险评估中保证对不同群体的公平对待。
- **医疗诊断**：确保对不同年龄、性别、种族的患者公平诊断。

公平性反向传播通过在训练过程中动态调整模型，能够显著降低模型的偏差，为各种应用场景提供更为公平的预测结果。
<br />
<br />
<br />
<br />

# 3. GNN-图神经网络
GNN（Graph Neural Network，图神经网络）是一类专为处理图结构数据而设计的深度学习模型。在图数据中，节点和边表示实体和它们之间的关系，而GNN能够通过学习这些**节点和边的关系**来提取复杂图结构的特征。这一技术在推荐系统、知识图谱、社交网络分析、交通网络、化学和生物学等领域中有着广泛应用。

### GNN的核心概念

1. **图的结构表示**：图包含节点（Nodes）和边（Edges），每个节点和边都可以带有特征信息。
   - 例如，社交网络中的图结构里，节点表示用户，边表示用户之间的关系；分子结构中，节点代表原子，边表示化学键。

2. **消息传递（Message Passing）**：GNN的关键在于消息传递机制，节点会从邻居节点接收信息并整合，从而更新自身的特征表示。这一过程是递归的，通过多层消息传递，每个节点逐渐整合多跳（多层邻居）节点的信息，以获得丰富的图上下文表示。

3. **节点表示更新**：在每一层消息传递过程中，GNN通过聚合（Aggregate）邻居节点的特征，并用组合（Combine）函数更新当前节点的表示。常见的聚合方法包括求和、平均、最大池化等。

4. **全局图表示**：在某些任务中，如图分类，需要对整个图进行表示，这时会将所有节点的信息汇总成一个全局向量，用于表征整个图。

### 主要的GNN架构

1. **GCN（Graph Convolutional Network，图卷积网络）**：GCN应用图卷积操作，通过加权求和的方式聚合邻居节点特征，生成新的节点嵌入。
   
2. **GAT（Graph Attention Network，图注意力网络）**：GAT利用注意力机制对邻居节点信息加权，使模型关注重要邻居，提高信息聚合的准确性。
   
3. **GraphSAGE（Graph Sample and Aggregate）**：GraphSAGE使用采样来限制邻居节点数目，从而在超大规模图上提高效率。
   
4. **R-GCN（Relational Graph Convolutional Network）**：R-GCN针对异构图（如知识图谱）设计，通过不同关系类型的边来处理图结构，特别适合知识图谱中的链接预测。

### GNN的应用领域

1. **社交网络分析**：用于社区检测、影响力分析等任务。
2. **推荐系统**：基于用户和项目的交互图，为用户提供个性化推荐。
3. **知识图谱**：用于链接预测和关系推理。
4. **分子特性预测**：如分子属性预测、药物活性预测等。
5. **交通网络**：GNN可用于交通流预测、优化路线和拥堵管理。

### 优势与挑战

**优势**：
- 能够捕捉图结构的复杂关系和依赖，具备很好的特征表示能力。
- 支持异构图、动态图建模，有较高的扩展性。

**挑战**：
- **计算成本高**：图越大，计算需求越高。
- **过平滑问题**：随着GNN层数增加，节点嵌入可能趋于一致，导致表达能力下降。
- **稀疏性问题**：图数据中可能存在孤立节点或稀疏连接，影响消息传递效果。

总之，GNN在捕获图结构信息方面独具优势，但仍在不断研究和优化中，以解决其计算效率、表达能力等问题。